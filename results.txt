\subsection{Computational Performance: run times and memory usage}


To asses the computational performance of AMplus, we conducted a large simulation study. 
We were interested in  the impact of study size on  performance so we generated data under five different scenarios. 
These are a GWAS of size 150 individuals and 5000 snp (150 X 5K), 350 individuals and 400000 snp (350 X 400K),  1500 individuals and 
50000 snp (1500 x 50K), 2000 individuals and 500000 snp (2000 x 500K), 4000 individuals and 
1500000 snp (4000 x 1.5M), and 10000 individuals and 1500000 snp (10000 x 1.5M).  
We chose these scenarios to reflect some of the different sized GWAS being performed in animals, plants, and humans. 
For each scenario, we generated 100 replicates of data. A single replicate consists of snp genotype data and quantitative trait data. 
We obtained the snp data from the publicly available 1000 genome project (phase 3). The quantitative trait data we generated from the 
snp data by selecting a set of snp loci, assigning allelic effects to these snp, and aggregating these effects for each individual along with 
random error. The number of snp selected per replicate follows a Poisson distribution with mean 30.
 The quantitative trait was generated to have a heritability of 50\%.
Analyses were performed on a high end desktop computer. It had dual 8-core Xeon processors, three Kepler Tesla GPUs, and 128 Giggabytes of RAM. All implementations except, GEMMA, made use of distributed computing, either explicitly or implicitly through multi-threaded BLAS/LAPACK libraries. 

\begin{figure}
\label{fig_runtimes}
\caption{Run times}
\includegraphics[width=10cm]{time}
\end{figure}


The run times for AMplus against the other software programs, across the five scenarios, is shown in figure \ref{fig_runtimes} To help with interpretibility of the results, in both plots, we have taken the x and y axes to be on a log scale. This means a unit change on the x or y axes is equivalent to a change in the order of magnitude. 
In the top plot, a point is the median of the ratios of elapse times of the multiple locus method to AMplus for a given scenario. The median is over the 100 replicates. Here, since the median of the ratios are all positive on the log scale, it means that AMplus had a shorter run time than than all the other multiple locus methods. In fact, in some cases, AMplus was over a hundred times, or over two orders of magnitude, faster. Unlike AMplus, the size of data under scenario 10000 X 1.5M was beyond the memory constraints of the other multiple locus implementations. This was also the case for LMM-Lasso, bigRR, and glmnet, for scenario 4000 x 1.5M.

In the bottom plot of figure \ref{fig_runtimes}, we compare the median run time of AMplus against the median run times of the single locus methods, FaST-LMM and GEMMA. 
FaST-LMM was run in two ways. It was run where the genetic similarity matrix was built with all the marker data (FaST-LMM$^all$) or with data on every 500th snp (FaST-LMM$^few$). 
AMplus was also run in two ways.  The default behavior for AMplus is to make use of CPUs for computing. However, AMplus also has the capacity to harness multiple GPUs (AMplus$^GPU$). 
From the bottom plot, all the implementations have short run times when analysing data from scenario 150x5K. However, for the other scenarios, AMplus and AMplus$GPU$ have 
significantly shorter run times than GEMMA and FaST-LMM$^all$. FaST-LMM$^few$ has a shorter run time than AMplus and AMplus$^GPU$ but only for scenarios 1500x50K and 
10000x1.5M.  Furthermore, for scenario 10000x1.5M, the median run time for GEMMA is 4071 minutes, for AMplus is 699 minutes, for AMplus$^GPU$ is 447 minutes, and for 
FaST-LMM$^few$ is 346 minutes. The very fast single-locus method FaST-LMM$^few$ is only 29\% faster than our multiple-locus method AMplus$^GPU$. It is worth noting though that 
there is a setup cost to accessing GPU computing, making AMplus$^GPU$ most efficient on the larger data. 

The memory usage of the software programs is shown in Figure \ref{fig_memory}. 

\begin{figure}
\label{fig_memory}
\caption{Memory usage}
\includegraphics[width=10cm]{memory}
\end{figure}




\subsection{Statistical Performance: power and FDR}


\begin{figure}
\label{fig_power}
\caption{Power curves}
\includegraphics[width=10cm]{power}
\end{figure}



\subsection{Application to Arabidopsis data}


\end{document}
